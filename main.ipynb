{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train_model\n",
    "from utils import load_config, load_model, inference\n",
    "from utils import visualize_output, get_keypoints_from_heatmaps\n",
    "import cv2\n",
    "import numpy as np\n",
    "from dataset import PoseDataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config('./config_w48_384x288.yaml')\n",
    "config['model']['weights'] = 'runs/20240918_205337/checkpoint_epoch_5/weights_epoch_5.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PoseDataset(config[\"dataset\"], config[\"dataset\"]['val'])\n",
    "# get one sample from the dataset\n",
    "# image,target,keypoints_gt,keypoint_visibility = dataset[334]\n",
    "# image,target,keypoints_gt,keypoint_visibility = dataset[632]\n",
    "image,target,keypoints_gt,keypoint_visibility = dataset[191]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = cv2.imread(\"./example_images/demo_cropped.jpg\")\n",
    "# image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "# image = cv2.resize(image, (384, 288))\n",
    "# image = image.transpose(2, 0, 1)\n",
    "# image = image.astype(np.float32)\n",
    "# image /= 255.0\n",
    "\n",
    "# image = torch.tensor(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = inference(config, image.unsqueeze(0))\n",
    "\n",
    "keypoints = get_keypoints_from_heatmaps(result, (image.shape[1], image.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config('./config_w48_384x288.yaml')\n",
    "result = inference(config, image.unsqueeze(0))\n",
    "\n",
    "keypoints_org = get_keypoints_from_heatmaps(result, (image.shape[1], image.shape[2]))\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_show = image.permute(1,2,0).numpy()\n",
    "# image_show = cv2.cvtColor(image_show, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "\n",
    "for i in range(keypoints_org.shape[1]):\n",
    "    cv2.circle(image_show, (int(keypoints_org[0][i][0]), int(keypoints_org[0][i][1])), 3, (0,0,255), -1)\n",
    "\n",
    "for i in range(len(keypoints_gt)):\n",
    "    cv2.circle(image_show, (int(keypoints_gt[i][0]), int(keypoints_gt[i][1])), 3, (0,255,0), -1)\n",
    "\n",
    "for i in range(keypoints.shape[1]):\n",
    "    cv2.circle(image_show, (int(keypoints[0][i][0]), int(keypoints[0][i][1])), 2, (255,0,0), -1)\n",
    "\n",
    "# add labels\n",
    "cv2.putText(image_show, 'Ground Truth', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1)\n",
    "cv2.putText(image_show, 'Predicted', (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 0, 0), 1)\n",
    "cv2.putText(image_show, 'Predicted Org', (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 0, 255), 1)\n",
    "\n",
    "# show image in notebook cell with matplotlib\n",
    "plt.imshow(image_show)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/cropped.csv')\n",
    "\n",
    "widths = df['box_w']\n",
    "heights = df['box_h']\n",
    "\n",
    "print(widths.mean(), heights.mean())\n",
    "print(min(widths), max(widths))\n",
    "print(min(heights), max(heights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(\"./data/cropped/cropped_C1S1A1D1_0818.jpg\")\n",
    "\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "# size = (576, 768)\n",
    "# size = (288, 384)\n",
    "size = (192, 256)\n",
    "image = cv2.resize(image, size)\n",
    "\n",
    "# config = load_config('./config_w48_384x288.yaml')\n",
    "# config = load_config('./config_ViT_B_simple.yaml')\n",
    "config = load_config('./config_ViT_B_classic.yaml')\n",
    "\n",
    "image = torch.tensor(image).float()\n",
    "image /= 255.0\n",
    "image = image.permute(2,0,1)\n",
    "image = image.unsqueeze(0)\n",
    "\n",
    "result = inference(config, image)\n",
    "\n",
    "keypoints = get_keypoints_from_heatmaps(result, (image.shape[2], image.shape[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "heatmaps = result[0].cpu().detach().numpy()\n",
    "\n",
    "image_show = cv2.imread(\"./data/cropped/cropped_C1S1A1D1_0818.jpg\")\n",
    "image_show = cv2.cvtColor(image_show, cv2.COLOR_BGR2RGB)\n",
    "image_show = cv2.resize(image_show, size)\n",
    "\n",
    "heatmap = np.sum(heatmaps, axis=0)\n",
    "heatmap = cv2.resize(heatmap, size)\n",
    "# heatmap = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "heatmap *= 255\n",
    "heatmap = heatmap.astype(np.uint8)\n",
    "heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_VIRIDIS)\n",
    "image_show = cv2.addWeighted(heatmap, 0.5, image_show, 0.5, 0)\n",
    "\n",
    "for i in range(keypoints.shape[1]):\n",
    "    cv2.circle(image_show, (int(keypoints[0][i][0]), int(keypoints[0][i][1])), 3, (0,0,255), -1)\n",
    "\n",
    "\n",
    "plt.imshow(image_show)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise different keypoint sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset import PoseDataset\n",
    "import torch\n",
    "from utils import load_config, load_model, inference, draw_keypoints\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "config = load_config('./configs/config_w32_256x192.yaml')\n",
    "img_size = (300, 400)\n",
    "config['dataset']['preprocess']['input_size'] = img_size\n",
    "\n",
    "config['dataset']['keypoints'] = 'small_17'\n",
    "dataset_17 = PoseDataset(config[\"dataset\"], config[\"dataset\"]['val'])\n",
    "\n",
    "config['dataset']['keypoints'] = 'med_34'\n",
    "dataset_34 = PoseDataset(config[\"dataset\"], config[\"dataset\"]['val'])\n",
    "\n",
    "config['dataset']['keypoints'] = 'all_63'\n",
    "dataset_63 = PoseDataset(config[\"dataset\"], config[\"dataset\"]['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_keypoints(dataset, n, color=(0,255,0)):\n",
    "    image, target, keypoints_gt, keypoint_visibility = dataset[n]\n",
    "    keypoints_gt = torch.tensor(keypoints_gt)\n",
    "    keypoint_visibility = torch.tensor(keypoint_visibility)\n",
    "    image_show = image.permute(1,2,0).numpy()\n",
    "    image_show *= 255\n",
    "    image_show = image_show.astype(np.uint8)\n",
    "    image_show = cv2.cvtColor(image_show, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    image_show = draw_keypoints(image_show, keypoints_gt, keypoint_visibility, color)\n",
    "\n",
    "    return image_show.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_f = 3174\n",
    "n_b = 2540\n",
    "img1 = show_keypoints(dataset_17, n_f, (0,255,0))\n",
    "img2 = show_keypoints(dataset_17, n_b, (0,255,0))\n",
    "img = np.append(img1, img2, axis=0)\n",
    "cv2.imwrite('small.jpg', img)\n",
    "\n",
    "img1 = show_keypoints(dataset_34, n_f, (255,0,0))\n",
    "img2 = show_keypoints(dataset_34, n_b, (255,0,0))\n",
    "img = np.append(img1, img2, axis=0)\n",
    "cv2.imwrite('med.jpg', img)\n",
    "\n",
    "img1 = show_keypoints(dataset_63, n_f, (0,0,255))\n",
    "img2 = show_keypoints(dataset_63, n_b, (0,0,255))\n",
    "img = np.append(img1, img2, axis=0)\n",
    "cv2.imwrite('all.jpg', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLO eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_config, normalized_mae_in_pixels\n",
    "from dataset import PoseDataset\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "config = load_config('./configs/config_w32_256x192.yaml')\n",
    "# img_size = (640, 640)\n",
    "img_size = (480, 640)\n",
    "# img_size = (960, 1280)\n",
    "# img_size = (192, 256)\n",
    "config['dataset']['preprocess']['input_size'] = img_size\n",
    "\n",
    "n = 64\n",
    "dataset = PoseDataset(config[\"dataset\"], config[\"dataset\"]['val'])\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=n, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = YOLO('./models/YOLO/runs/pose/train9/weights/best.pt').to(device)\n",
    "# model = YOLO('./weights/yolov8x-pose.pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keypoints_yolo(outputs, num_keypoints=17):\n",
    "    keypoints = []\n",
    "    for output in outputs:\n",
    "        xy = output.keypoints.xy[0]\n",
    "        if xy.shape[0] < num_keypoints:\n",
    "            xy = torch.cat((xy, torch.zeros(num_keypoints - xy.shape[0], 2).to(xy.device)))\n",
    "        keypoints.append(xy)\n",
    "    \n",
    "    return torch.stack(keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_mae = 0\n",
    "keypoints = None\n",
    "with torch.no_grad():\n",
    "    for images, targets, gt_keypoints, keypoint_visibility in tqdm.tqdm(loader):\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        gt_keypoints = gt_keypoints.to(device)\n",
    "        keypoint_visibility = keypoint_visibility.to(device)\n",
    "        outputs = model(images, verbose=False)\n",
    "        keypoints = get_keypoints_yolo(outputs, num_keypoints=17)\n",
    "        norm_mae += normalized_mae_in_pixels(keypoints, gt_keypoints, img_size, keypoint_visibility)\n",
    "    \n",
    "norm_mae /= len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(norm_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 20\n",
    "\n",
    "image_show = images[n].permute(1,2,0).cpu().numpy().copy()\n",
    "\n",
    "keypoints_show = keypoints[n].cpu().numpy()\n",
    "keypoints_gt = gt_keypoints[n].cpu().numpy()\n",
    "\n",
    "plt.imshow(image_show)\n",
    "plt.plot(keypoints_show[:,0], keypoints_show[:,1], 'bo', markersize=3)\n",
    "plt.plot(keypoints_gt[:,0], keypoints_gt[:,1], 'go', markersize=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping final_layer.weight due to shape mismatch: Error(s) in loading state_dict for PoseHighResolutionNet:\n",
      "\tsize mismatch for final_layer.weight: copying a param with shape torch.Size([17, 48, 1, 1]) from checkpoint, the shape in current model is torch.Size([34, 48, 1, 1]).\n",
      "Skipping final_layer.bias due to shape mismatch: Error(s) in loading state_dict for PoseHighResolutionNet:\n",
      "\tsize mismatch for final_layer.bias: copying a param with shape torch.Size([17]) from checkpoint, the shape in current model is torch.Size([34]).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time: 6.547427177429199 ms\n"
     ]
    }
   ],
   "source": [
    "from utils import load_config, load_model\n",
    "import time\n",
    "from dataset import PoseDataset\n",
    "import torch\n",
    "\n",
    "config = load_config('./configs/config_w48_768x576.yaml')\n",
    "\n",
    "\n",
    "n = 10\n",
    "images = torch.randn(n, 3, 768, 576)\n",
    "\n",
    "device = 'cuda:0'\n",
    "device = torch.device(device)\n",
    "\n",
    "model = load_model(config['model']).to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    images = images.to(device)\n",
    "    start = time.time()\n",
    "    outputs = model(images)\n",
    "    end = time.time()\n",
    "    avg_time = ((end - start) / n) * 1000\n",
    "\n",
    "\n",
    "print(f'Average inference time: {avg_time} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 1280x1280 (no detections), 28.6ms\n",
      "1: 1280x1280 (no detections), 28.6ms\n",
      "2: 1280x1280 (no detections), 28.6ms\n",
      "3: 1280x1280 (no detections), 28.6ms\n",
      "4: 1280x1280 (no detections), 28.6ms\n",
      "5: 1280x1280 (no detections), 28.6ms\n",
      "6: 1280x1280 (no detections), 28.6ms\n",
      "7: 1280x1280 (no detections), 28.6ms\n",
      "8: 1280x1280 (no detections), 28.6ms\n",
      "9: 1280x1280 (no detections), 28.6ms\n",
      "Speed: 0.0ms preprocess, 28.6ms inference, 0.6ms postprocess per image at shape (1, 3, 1280, 1280)\n",
      "Average inference time: 105.18145561218262 ms\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import statistics\n",
    "import torch\n",
    "import time\n",
    "\n",
    "model = YOLO('./weights/yolov8x-pose-p6.pt')\n",
    "\n",
    "device = 'cuda:0'\n",
    "device = torch.device(device)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "n = 10\n",
    "\n",
    "# generate random images\n",
    "images = torch.rand((n, 3, 1280, 1280))\n",
    "\n",
    "# measure inference time\n",
    "with torch.no_grad():\n",
    "    images = images.to(device)\n",
    "    start = time.time()\n",
    "    outputs = model(images)\n",
    "    end = time.time()\n",
    "    avg_time = ((end - start) / n) * 1000\n",
    "\n",
    "print(f'Average inference time: {avg_time} ms')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
