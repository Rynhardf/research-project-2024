{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import PoseDataset\n",
    "from utils import load_config, load_model\n",
    "from train import validate\n",
    "from utils import JointsMSELoss\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import get_keypoints_from_heatmaps, compute_oks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config('./configs/config_w48_384x288.yaml')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_size = config[\"dataset\"][\"preprocess\"][\"input_size\"]\n",
    "batch_size = config[\"training\"][\"batch_size\"]\n",
    "\n",
    "run = '20240930_104432'\n",
    "epoch = 1\n",
    "config['model']['weights'] = f'runs/{run}/checkpoint_epoch_{epoch}/weights_epoch_{epoch}.pth'\n",
    "\n",
    "model = load_model(config['model'])\n",
    "model = model.to(device)\n",
    "\n",
    "val_dataset = PoseDataset(config[\"dataset\"], config[\"dataset\"]['val'])\n",
    "\n",
    "val_loader = DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=False, num_workers=8\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average OKS: 0.7557495832443237\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "avg_oks = 0\n",
    "sigmas = torch.ones(34) * 0.03\n",
    "sigmas = sigmas.to(device)\n",
    "with torch.no_grad():\n",
    "    for images, targets, gt_keypoints, keypoint_visibility,bbox in val_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        gt_keypoints = gt_keypoints.to(device)\n",
    "        keypoint_visibility = keypoint_visibility.to(device)\n",
    "        bbox = bbox.to(device)\n",
    "        outputs = model(images)\n",
    "        pred_keypoints = get_keypoints_from_heatmaps(\n",
    "            outputs.detach(), input_size[::-1]\n",
    "        )\n",
    "\n",
    "        # area = (bbox[:, 2] - bbox[:, 0]) * (bbox[:, 3] - bbox[:, 1])\n",
    "        area = torch.ones(bbox.shape[0]) * images.shape[2] * images.shape[3]\n",
    "        area = area.to(device)\n",
    "        oks = compute_oks(pred_keypoints, gt_keypoints, keypoint_visibility, sigmas, area)\n",
    "        avg_oks += oks\n",
    "\n",
    "print(f'Average OKS: {avg_oks / len(val_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w48-384x288: 0.8221208624383236\n",
    "w32-256x192: 0.7493196738527176\n",
    "w48_768x576: 0.8510695039269759\n",
    "ViT-Pose-B-simple: 0.5001742104266552\n",
    "ViT-Pose-B-classic: 0.7557495832443237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP: 0.9000\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "sigmas = torch.ones(17) * 0.03\n",
    "sigmas = sigmas.to(device)\n",
    "\n",
    "# Define OKS thresholds (similar to COCO: 0.50 to 0.95 with a step of 0.05)\n",
    "oks_thresholds = torch.arange(0.5, 1.0, 0.05).to(device)\n",
    "\n",
    "# Store precision at each threshold\n",
    "precisions = torch.zeros(len(oks_thresholds)).to(device)\n",
    "\n",
    "batch_count = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, targets, gt_keypoints, keypoint_visibility, bbox in val_loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        gt_keypoints = gt_keypoints.to(device)\n",
    "        keypoint_visibility = keypoint_visibility.to(device)\n",
    "        bbox = bbox.to(device)\n",
    "        \n",
    "        # Model inference\n",
    "        outputs = model(images)\n",
    "        pred_keypoints = get_keypoints_from_heatmaps(\n",
    "            outputs.detach(), input_size[::-1]\n",
    "        )\n",
    "        \n",
    "        # Compute area (bbox can be used, or image size as in your original code)\n",
    "        area = torch.ones(bbox.shape[0]) * images.shape[2] * images.shape[3]\n",
    "        area = area.to(device)\n",
    "        \n",
    "        # Compute OKS values for the current batch\n",
    "        oks = compute_oks(pred_keypoints, gt_keypoints, keypoint_visibility, sigmas, area)\n",
    "\n",
    "        # For each OKS threshold, check if OKS is above the threshold (correct prediction)\n",
    "        for i, threshold in enumerate(oks_thresholds):\n",
    "            precisions[i] += (oks >= threshold).float().mean()\n",
    "        \n",
    "        batch_count += 1\n",
    "\n",
    "precisions /= batch_count\n",
    "\n",
    "# Compute mean Average Precision (mAP)\n",
    "map_value = precisions.mean().item()\n",
    "\n",
    "print(f'mAP: {map_value:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "34\n",
    "w32-256x192: 0.5383\n",
    "w48-384x288: 0.7000\n",
    "w48_768x576: 0.7548\n",
    "ViT-Pose-B-classic: 0.5894\n",
    "\n",
    "17\n",
    "w48-384x288: 0.7032"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
